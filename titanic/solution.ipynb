{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, csv_file, train=True):\n",
    "        self.train = train\n",
    "        \n",
    "        df = pd.read_csv(csv_file, header=0)\n",
    "\n",
    "        def proc_cabin(cabin_str):\n",
    "            if pd.isna(cabin_str):\n",
    "                return '?'\n",
    "            return cabin_str[0]\n",
    "\n",
    "        df['cabin_proc'] = df['Cabin'].apply(proc_cabin)        \n",
    "\n",
    "        fare_mean = df['Fare'].median()\n",
    "\n",
    "        def proc_fare(fare_val):\n",
    "            if fare_val == 0:\n",
    "                return fare_mean\n",
    "            return fare_val\n",
    "\n",
    "        df['fare_proc'] = df['Fare'].apply(proc_fare)\n",
    "        \n",
    "        age_mean = df['Age'].mean()\n",
    "        df['age_proc'] = df[['Age']].fillna(age_mean)\n",
    "\n",
    "        df2 = pd.concat([\n",
    "            df,\n",
    "            pd.get_dummies(df['Pclass'], prefix='pclass'), \n",
    "            pd.get_dummies(df['Sex']),\n",
    "            pd.get_dummies(df['Embarked'], prefix='embarked'),\n",
    "            pd.get_dummies(df['cabin_proc'], prefix='cabin'),\n",
    "        ], axis='columns')\n",
    "\n",
    "        features = [\n",
    "            'age_proc', 'SibSp', 'Parch', 'pclass_1', 'pclass_2', 'pclass_3', \n",
    "            'female', 'male',\n",
    "            'embarked_C', 'embarked_Q', 'embarked_S', 'fare_proc',\n",
    "            'cabin_?', 'cabin_A', 'cabin_B', 'cabin_C', 'cabin_D', 'cabin_E', 'cabin_F', 'cabin_G'\n",
    "        ]\n",
    "\n",
    "        df3 = df2[features]\n",
    "        \n",
    "        df3_scaled = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df3))\n",
    "        tensor = torch.tensor(df3_scaled.values)\n",
    "        \n",
    "        self.tensor_inputs = tensor.float()\n",
    "        self.tensor_ids = torch.tensor(df['PassengerId'].values)\n",
    "\n",
    "        if train:\n",
    "            df3_label = pd.get_dummies(df['Survived'], prefix='survived')\n",
    "            self.tensor_labels = torch.tensor(df3_label.values).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tensor_inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.tensor_ids[idx], self.tensor_inputs[idx], self.tensor_labels[idx]\n",
    "        else:\n",
    "            return self.tensor_ids[idx], self.tensor_inputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btsai/miniconda3/envs/mytorch/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/btsai/miniconda3/envs/mytorch/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "dataset = TitanicDataset('data/train.csv')\n",
    "submit_dataset = TitanicDataset('data/test.csv', False)\n",
    "\n",
    "feature_size = len(dataset[0][1])\n",
    "\n",
    "batch_size = 25\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "submit_loader = torch.utils.data.DataLoader(submit_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(feature_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=20, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n",
      "[1] loss: 0.230\n",
      "[11] loss: 0.168\n",
      "[21] loss: 0.164\n",
      "[31] loss: 0.161\n",
      "[41] loss: 0.161\n",
      "[51] loss: 0.159\n",
      "[61] loss: 0.160\n",
      "[71] loss: 0.159\n",
      "[81] loss: 0.159\n",
      "[91] loss: 0.158\n",
      "[101] loss: 0.157\n",
      "[111] loss: 0.157\n",
      "[121] loss: 0.157\n",
      "[131] loss: 0.157\n",
      "[141] loss: 0.156\n",
      "[151] loss: 0.156\n",
      "[161] loss: 0.155\n",
      "[171] loss: 0.154\n",
      "[181] loss: 0.155\n",
      "[191] loss: 0.155\n",
      "[201] loss: 0.155\n",
      "[211] loss: 0.155\n",
      "[221] loss: 0.153\n",
      "[231] loss: 0.153\n",
      "[241] loss: 0.153\n",
      "[251] loss: 0.153\n",
      "[261] loss: 0.152\n",
      "[271] loss: 0.153\n",
      "[281] loss: 0.152\n",
      "[291] loss: 0.152\n",
      "[301] loss: 0.152\n",
      "[311] loss: 0.153\n",
      "[321] loss: 0.152\n",
      "[331] loss: 0.152\n",
      "[341] loss: 0.151\n",
      "[351] loss: 0.151\n",
      "[361] loss: 0.151\n",
      "[371] loss: 0.150\n",
      "[381] loss: 0.151\n",
      "[391] loss: 0.151\n",
      "[401] loss: 0.150\n",
      "[411] loss: 0.150\n",
      "[421] loss: 0.150\n",
      "[431] loss: 0.150\n",
      "[441] loss: 0.151\n",
      "[451] loss: 0.150\n",
      "[461] loss: 0.150\n",
      "[471] loss: 0.150\n",
      "[481] loss: 0.150\n",
      "[491] loss: 0.152\n",
      "[501] loss: 0.150\n",
      "[511] loss: 0.150\n",
      "[521] loss: 0.150\n",
      "[531] loss: 0.151\n",
      "[541] loss: 0.151\n",
      "[551] loss: 0.151\n",
      "[561] loss: 0.152\n",
      "[571] loss: 0.150\n",
      "[581] loss: 0.150\n",
      "[591] loss: 0.150\n",
      "[601] loss: 0.150\n",
      "[611] loss: 0.152\n",
      "[621] loss: 0.150\n",
      "[631] loss: 0.150\n",
      "[641] loss: 0.150\n",
      "[651] loss: 0.150\n",
      "[661] loss: 0.149\n",
      "[671] loss: 0.150\n",
      "[681] loss: 0.150\n",
      "[691] loss: 0.150\n",
      "[701] loss: 0.151\n",
      "[711] loss: 0.151\n",
      "[721] loss: 0.150\n",
      "[731] loss: 0.151\n",
      "[741] loss: 0.150\n",
      "[751] loss: 0.150\n",
      "[761] loss: 0.150\n",
      "[771] loss: 0.150\n",
      "[781] loss: 0.149\n",
      "[791] loss: 0.150\n",
      "[801] loss: 0.150\n",
      "[811] loss: 0.149\n",
      "[821] loss: 0.149\n",
      "[831] loss: 0.149\n",
      "[841] loss: 0.150\n",
      "[851] loss: 0.150\n",
      "[861] loss: 0.149\n",
      "[871] loss: 0.150\n",
      "[881] loss: 0.150\n",
      "[891] loss: 0.149\n",
      "[901] loss: 0.150\n",
      "[911] loss: 0.149\n",
      "[921] loss: 0.151\n",
      "[931] loss: 0.150\n",
      "[941] loss: 0.150\n",
      "[951] loss: 0.150\n",
      "[961] loss: 0.149\n",
      "[971] loss: 0.150\n",
      "[981] loss: 0.151\n",
      "[991] loss: 0.150\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    records = 0\n",
    "    for data in train_loader:\n",
    "\n",
    "        _, inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        inputs = inputs[:,:,None]\n",
    "        inputs = inputs.permute(0,2,1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs[:,0], torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        records += len(data)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"[%d] loss: %.3f\" % (epoch + 1, running_loss / records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623595505617978\n",
      "0.8324022346368715\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            _, inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            inputs = inputs[:,:,None]\n",
    "            inputs = inputs.permute(0,2,1)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            predicted = torch.max(outputs[:,0], 1)[1]\n",
    "            expected = torch.max(labels, 1)[1]\n",
    "\n",
    "            total += predicted.size(0)\n",
    "            correct += (predicted == expected).sum().item()\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "calculate_accuracy(train_loader)\n",
    "calculate_accuracy(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(dataloader):\n",
    "    res = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            ids, inputs = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            inputs = inputs[:,:,None]\n",
    "            inputs = inputs.permute(0,2,1)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            predicted = torch.max(outputs[:,0], 1)[1].to('cpu')\n",
    "            \n",
    "            res += list(zip(ids.numpy(), predicted.numpy()))\n",
    "    return res\n",
    "            \n",
    "res = get_predictions(submit_loader)\n",
    "df_res = pd.DataFrame(res, columns=['PassengerId', 'Survived'])\n",
    "df_res.to_csv(path_or_buf='data/predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
